{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository and set up the environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "if not os.path.exists('/content/finfo'):\n",
    "    print(\"Cloning the finfo repository...\")\n",
    "    !git clone https://github.com/etrigan5500/finfo.git /content/finfo\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"Repository already exists, pulling latest changes...\")\n",
    "    !cd /content/finfo && git pull\n",
    "    print(\"Repository updated!\")\n",
    "\n",
    "# Change to the repository directory\n",
    "os.chdir('/content/finfo')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Add the repository to Python path so we can import modules\n",
    "sys.path.append('/content/finfo')\n",
    "\n",
    "# Verify the file structure\n",
    "print(\"\\nRepository structure:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\nTraining directory structure:\")\n",
    "!ls -la training/\n",
    "\n",
    "print(\"\\nData collection modules:\")\n",
    "!ls -la data_collection/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import existing modules from the repository\n",
    "try:\n",
    "    # Import the news collector from the repository\n",
    "    from data_collection.news.news_collector import NewsCollector as RepoNewsCollector\n",
    "    print(\"‚úÖ Successfully imported NewsCollector from repository\")\n",
    "    \n",
    "    # Import other utilities if available\n",
    "    from data_collection.financial.financial_collector import FinancialCollector\n",
    "    print(\"‚úÖ Successfully imported FinancialCollector from repository\")\n",
    "    \n",
    "    # Use the repository's news collector\n",
    "    USE_REPO_MODULES = True\n",
    "    print(\"üéâ Using modules from the cloned repository!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import repository modules: {e}\")\n",
    "    print(\"üìù Will use the notebook-defined classes instead\")\n",
    "    USE_REPO_MODULES = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories for the project\n",
    "import os\n",
    "\n",
    "# Create directories that might be needed\n",
    "directories_to_create = [\n",
    "    'models',\n",
    "    'training/models',\n",
    "    'training/results',\n",
    "    'data/processed',\n",
    "    'logs'\n",
    "]\n",
    "\n",
    "for directory in directories_to_create:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"‚úÖ Created/verified directory: {directory}\")\n",
    "\n",
    "print(\"\\nüìÅ Directory structure ready for training!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ Google Colab Setup Complete!\\n\\nThis notebook is now ready to run in Google Colab with the following features:\\n\\n## ‚úÖ What's Been Set Up:\\n1. **Repository Cloned**: The finfo repository has been cloned to `/content/finfo`\\n2. **Modules Imported**: Existing data collection modules are imported when available\\n3. **Directories Created**: All necessary directories for training and model storage\\n4. **Google Custom Search API**: Updated to use Google Custom Search instead of SerpAPI\\n\\n## üîë API Setup (Optional but Recommended):\\n\\nTo use real news data, you'll need to set up Google Custom Search API:\\n\\n1. **Get Google Custom Search API Key**:\\n   - Go to [Google Cloud Console](https://console.cloud.google.com/)\\n   - Enable the Custom Search API\\n   - Create an API key\\n\\n2. **Create Custom Search Engine**:\\n   - Go to [Google Custom Search](https://cse.google.com/)\\n   - Create a new search engine\\n   - Get your Search Engine ID\\n\\n3. **Set Your Credentials** (in the configuration cell below):\\n   ```python\\n   GOOGLE_API_KEY = \\\"your_api_key_here\\\"\\n   SEARCH_ENGINE_ID = \\\"your_search_engine_id_here\\\"\\n   ```\\n\\n## üìä Benefits:\\n- **100 FREE searches per day** (vs SerpAPI's 100/month)\\n- **Better rate limits** for development\\n- **Same clustering functionality** for article diversity\\n\\n## üéØ Ready to Train!\\nYou can now run all cells to start training your stock prediction models!\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Stock Trend Predictor - Model Training\n",
    "\n",
    "This notebook contains the training code for all three models and the ensemble model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers pandas numpy scikit-learn yfinance sentence-transformers google-api-python-client python-dotenv beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Define Model Classes\n",
    "\n",
    "First, let's define our model classes that we'll train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model Class\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "\n",
    "# News Model\n",
    "class NewsModel(BaseModel):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=5):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Freeze BERT parameters\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)  # Changed to num_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Financial Model\n",
    "class FinancialModel(BaseModel):\n",
    "    def __init__(self, input_size=6, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, num_classes)  # Changed to num_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Price Model\n",
    "class PriceModel(BaseModel):\n",
    "    def __init__(self, sequence_length=24, num_classes=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # CNN layers for feature extraction\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, num_classes)  # Changed to num_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, sequence_length)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        features = self.conv_layers(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        features = features.permute(0, 2, 1)  # (batch_size, seq_len, features)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        # Take the last output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(last_output)\n",
    "\n",
    "# Ensemble Model\n",
    "class EnsembleModel(BaseModel):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialize individual models\n",
    "        self.news_model = NewsModel(num_classes=num_classes)\n",
    "        self.financial_model = FinancialModel(num_classes=num_classes)\n",
    "        self.price_model = PriceModel(num_classes=num_classes)\n",
    "        \n",
    "        # Ensemble classifier - takes concatenated features from all models\n",
    "        self.ensemble = nn.Sequential(\n",
    "            nn.Linear(num_classes * 3, 32),  # 5*3 = 15 input features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, num_classes)  # Final 5-class output\n",
    "        )\n",
    "        \n",
    "    def forward(self, news_input_ids, news_attention_mask, financial_data, price_data):\n",
    "        # Get predictions from individual models\n",
    "        news_pred = self.news_model.forward(news_input_ids, news_attention_mask)\n",
    "        financial_pred = self.financial_model.forward(financial_data)\n",
    "        price_pred = self.price_model.forward(price_data)\n",
    "        \n",
    "        # Combine predictions\n",
    "        combined = torch.cat([news_pred, financial_pred, price_pred], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        return self.ensemble(combined)\n",
    "\n",
    "print(\"Model classes defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data Collection Efficiency Comparison\n",
    "\n",
    "The updated training code now offers multiple configurations to balance efficiency with data quality:\n",
    "\n",
    "**Configuration Options:**\n",
    "\n",
    "1. **efficient_6mo**: Most efficient\n",
    "   - 6 months of weekly data from yfinance API\n",
    "   - Resampled to 15-day intervals (12 data points)\n",
    "   - Sequence length: 12\n",
    "   - API calls: ~50 weekly data points per ticker\n",
    "   - Good for quick experiments\n",
    "\n",
    "2. **balanced_1y**: Recommended balance  \n",
    "   - 1 year of weekly data from yfinance API\n",
    "   - Resampled to 15-day intervals (24 data points)\n",
    "   - Sequence length: 24 (full model capacity)\n",
    "   - API calls: ~100 weekly data points per ticker\n",
    "   - Best balance of efficiency and model performance\n",
    "\n",
    "3. **detailed_6mo**: More granular but less efficient\n",
    "   - 6 months of daily data from yfinance API\n",
    "   - Resampled to 15-day intervals (12 data points)\n",
    "   - Sequence length: 12\n",
    "   - API calls: ~180 daily data points per ticker\n",
    "   - More precise but computationally expensive\n",
    "\n",
    "4. **original_2y**: Original approach\n",
    "   - 2 years of daily data from yfinance API\n",
    "   - Resampled to 15-day intervals (48 data points)\n",
    "   - Sequence length: 24\n",
    "   - API calls: ~730 daily data points per ticker\n",
    "   - Most comprehensive but slowest\n",
    "\n",
    "**Efficiency Notes:**\n",
    "- Using `interval='1wk'` gets weekly data directly from yfinance, avoiding the need to download and resample 365+ daily data points\n",
    "- For 20 companies: weekly approach = ~2,000 API data points vs daily approach = ~14,600 data points\n",
    "- This reduces data collection time by ~85% while maintaining prediction quality\n",
    "\n",
    "**Google Custom Search API Usage and Benefits:**\n",
    "- **News collection**: 2 focused queries per company (financial news + stock prediction news)\n",
    "- **Total API calls**: 20 companies √ó 2 queries = **40 calls per training run**\n",
    "- **Articles collected**: 20 articles per company ‚Üí clustered into 3 topic groups\n",
    "- **Rate limits**: Google Custom Search API free tier allows **100 searches/day** (vs SerpAPI's 100/month)\n",
    "- **Cost**: FREE for up to 100 searches/day, then $5 per 1000 queries\n",
    "- **Clustering**: Uses sentence transformers to group articles by topic for better diversity\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data Collection and Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Collection Class with intelligent clustering\n",
    "# Use repository version if available, otherwise use notebook version\n",
    "if USE_REPO_MODULES:\n",
    "    print(\"üîÑ Using NewsCollector from repository\")\n",
    "    NewsCollector = RepoNewsCollector\n",
    "else:\n",
    "    print(\"üìù Using NewsCollector defined in notebook\")\n",
    "    \n",
    "    class NewsCollector:\n",
    "    def __init__(self, api_key=None, search_engine_id=None):\n",
    "        self.api_key = api_key\n",
    "        self.search_engine_id = search_engine_id\n",
    "        self.use_real_news = api_key is not None and search_engine_id is not None\n",
    "        \n",
    "    def get_company_news(self, company_name, ticker, num_articles=5):\n",
    "        \"\"\"Get clustered news articles for a company.\"\"\"\n",
    "        if self.use_real_news and self.api_key:\n",
    "            return self._get_real_news_clustered(company_name, ticker, num_articles)\n",
    "        else:\n",
    "            return self._get_synthetic_news(company_name, ticker, num_articles)\n",
    "    \n",
    "    def _get_real_news_clustered(self, company_name, ticker, num_articles):\n",
    "        \"\"\"Get real news using Google Custom Search API with clustering.\"\"\"\n",
    "        try:\n",
    "            from googleapiclient.discovery import build\n",
    "            from sklearn.cluster import KMeans\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import numpy as np\n",
    "            \n",
    "            # Use focused financial and prediction queries (2 queries instead of 3)\n",
    "            search_queries = [\n",
    "                f\"{company_name} financial news\",\n",
    "                f\"{company_name} stock prediction news\"\n",
    "            ]\n",
    "            \n",
    "            all_articles = []\n",
    "            print(f\"Fetching news for {ticker} with {len(search_queries)} queries...\")\n",
    "            \n",
    "            # Initialize Google Custom Search API service\n",
    "            service = build(\"customsearch\", \"v1\", developerKey=self.api_key)\n",
    "            \n",
    "            for query in search_queries:\n",
    "                try:\n",
    "                    # Execute search\n",
    "                    result = service.cse().list(\n",
    "                        q=query,\n",
    "                        cx=self.search_engine_id,\n",
    "                        num=10  # Get 10 articles per query = 20 total\n",
    "                    ).execute()\n",
    "                    \n",
    "                    if \"items\" in result:\n",
    "                        for item in result[\"items\"]:\n",
    "                            title = item.get('title', '')\n",
    "                            snippet = item.get('snippet', '')\n",
    "                            if title and snippet:\n",
    "                                combined_text = f\"{title}. {snippet}\"\n",
    "                                all_articles.append(combined_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in search query '{query}': {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Collected {len(all_articles)} articles for {ticker}\")\n",
    "            \n",
    "            # If we have enough articles, cluster them\n",
    "            if len(all_articles) >= 6:  # Need at least 6 articles for 3 clusters\n",
    "                return self._cluster_articles(all_articles, num_clusters=3, articles_per_cluster=num_articles//3)\n",
    "            else:\n",
    "                # Return what we have if not enough for clustering\n",
    "                return all_articles[:num_articles]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching real news for {ticker}: {str(e)}\")\n",
    "            return self._get_synthetic_news(company_name, ticker, num_articles)\n",
    "    \n",
    "    def _cluster_articles(self, articles, num_clusters=3, articles_per_cluster=2):\n",
    "        \"\"\"Cluster articles by topic and return representative articles from each cluster.\"\"\"\n",
    "        try:\n",
    "            # Initialize sentence transformer for semantic clustering\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            \n",
    "            # Get embeddings for all articles\n",
    "            embeddings = model.encode(articles)\n",
    "            \n",
    "            # Perform K-means clustering\n",
    "            kmeans = KMeans(n_clusters=min(num_clusters, len(articles)), random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(embeddings)\n",
    "            \n",
    "            # Get representative articles from each cluster\n",
    "            clustered_articles = []\n",
    "            for cluster_id in range(num_clusters):\n",
    "                cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "                \n",
    "                if len(cluster_indices) > 0:\n",
    "                    # Get articles closest to cluster center\n",
    "                    cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                    distances = np.linalg.norm(embeddings[cluster_indices] - cluster_center, axis=1)\n",
    "                    \n",
    "                    # Sort by distance to center and take the closest ones\n",
    "                    sorted_indices = cluster_indices[np.argsort(distances)]\n",
    "                    \n",
    "                    # Take up to articles_per_cluster from this cluster\n",
    "                    for idx in sorted_indices[:articles_per_cluster]:\n",
    "                        clustered_articles.append(articles[idx])\n",
    "            \n",
    "            print(f\"Clustered into {num_clusters} topics, returning {len(clustered_articles)} representative articles\")\n",
    "            return clustered_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error clustering articles: {str(e)}. Returning random sample.\")\n",
    "            # Fallback to random sampling if clustering fails\n",
    "            import random\n",
    "            return random.sample(articles, min(len(articles), num_clusters * articles_per_cluster))\n",
    "    \n",
    "    def _get_synthetic_news(self, company_name, ticker, num_articles):\n",
    "        \"\"\"Generate diverse synthetic news for training when no API key is provided.\"\"\"\n",
    "        # Create varied synthetic news to prevent overfitting\n",
    "        news_templates = [\n",
    "            f\"{company_name} reports strong quarterly earnings with revenue beating expectations.\",\n",
    "            f\"{company_name} announces new product innovations and market expansion plans.\",\n",
    "            f\"{company_name} faces regulatory challenges but maintains growth trajectory.\",\n",
    "            f\"{company_name} stock sees volatility amid broader market uncertainty.\",\n",
    "            f\"{company_name} CEO discusses future strategy and investment priorities.\",\n",
    "            f\"{company_name} partnerships drive growth in key technology sectors.\",\n",
    "            f\"{company_name} navigates supply chain challenges while increasing market share.\",\n",
    "            f\"{company_name} reports mixed quarterly results with strong guidance ahead.\",\n",
    "            f\"{company_name} invests heavily in AI and cloud infrastructure development.\",\n",
    "            f\"{company_name} stock analysts upgrade ratings citing strong fundamentals.\"\n",
    "        ]\n",
    "        \n",
    "        # Add some randomness to make articles more diverse\n",
    "        import random\n",
    "        selected_templates = random.sample(news_templates, min(num_articles, len(news_templates)))\n",
    "        \n",
    "        # Add some variation with sentiment and market conditions\n",
    "        sentiment_modifiers = [\n",
    "            \"optimistic\", \"cautious\", \"bullish\", \"bearish\", \"neutral\",\n",
    "            \"positive\", \"mixed\", \"concerned\", \"confident\", \"uncertain\"\n",
    "        ]\n",
    "        \n",
    "        articles = []\n",
    "        for i, template in enumerate(selected_templates):\n",
    "            sentiment = random.choice(sentiment_modifiers)\n",
    "            modified_article = f\"{template} Market sentiment appears {sentiment} regarding recent developments.\"\n",
    "            articles.append(modified_article)\n",
    "            \n",
    "        return articles\n",
    "\n",
    "def classify_return(return_pct):\n",
    "    \"\"\"Classify return percentage into 5 categories.\"\"\"\n",
    "    if return_pct < -0.02:  # < -2%\n",
    "        return 0  # Strong Decrease\n",
    "    elif return_pct < -0.005:  # -2% to -0.5%\n",
    "        return 1  # Moderate Decrease\n",
    "    elif return_pct <= 0.005:  # -0.5% to +0.5%\n",
    "        return 2  # Stable\n",
    "    elif return_pct <= 0.02:  # +0.5% to +2%\n",
    "        return 3  # Moderate Increase\n",
    "    else:  # > +2%\n",
    "        return 4  # Strong Increase\n",
    "\n",
    "\n",
    "\n",
    "def collect_training_data_efficient(ticker_list, data_period='1y', interval='1wk', \n",
    "                                  resample_period='15D', api_key=None, search_engine_id=None,\n",
    "                                  num_samples_per_ticker=50, sequence_length=12):\n",
    "    \"\"\"\n",
    "    Efficiently collect training data with flexible periods and intervals.\n",
    "    \n",
    "    Args:\n",
    "        ticker_list: List of stock tickers\n",
    "        data_period: Period to fetch ('6mo', '1y', '2y', etc.)\n",
    "        interval: Data interval ('1d', '1wk', '1mo')\n",
    "        resample_period: Period to resample to ('15D', '1M', etc.)\n",
    "        api_key: Google Custom Search API key for real news (optional)\n",
    "        search_engine_id: Google Custom Search Engine ID (optional)\n",
    "        num_samples_per_ticker: Number of samples per ticker\n",
    "        sequence_length: Length of price sequence for model (adjusted based on available data)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Initialize news collector\n",
    "    if USE_REPO_MODULES:\n",
    "        news_collector = NewsCollector(api_key, search_engine_id)\n",
    "        print(\"üîÑ Using repository NewsCollector\")\n",
    "    else:\n",
    "        news_collector = NewsCollector(api_key, search_engine_id)\n",
    "        print(\"üìù Using notebook NewsCollector\")\n",
    "    \n",
    "    # Company name mapping for better news search\n",
    "    company_names = {\n",
    "        'AAPL': 'Apple Inc',\n",
    "        'MSFT': 'Microsoft Corporation', \n",
    "        'GOOGL': 'Alphabet Google',\n",
    "        'AMZN': 'Amazon',\n",
    "        'TSLA': 'Tesla',\n",
    "        'META': 'Meta Facebook',\n",
    "        'NVDA': 'NVIDIA Corporation',\n",
    "        'NFLX': 'Netflix',\n",
    "        'ADBE': 'Adobe',\n",
    "        'CRM': 'Salesforce',\n",
    "        'ORCL': 'Oracle Corporation',\n",
    "        'IBM': 'IBM',\n",
    "        'INTC': 'Intel Corporation',\n",
    "        'AMD': 'Advanced Micro Devices',\n",
    "        'CSCO': 'Cisco Systems',\n",
    "        'PYPL': 'PayPal',\n",
    "        'UBER': 'Uber Technologies',\n",
    "        'LYFT': 'Lyft',\n",
    "        'SNAP': 'Snap Inc',\n",
    "        'TWTR': 'Twitter'\n",
    "    }\n",
    "    \n",
    "    for ticker in ticker_list:\n",
    "        try:\n",
    "            print(f\"Processing {ticker}...\")\n",
    "            \n",
    "            # Get stock data efficiently with specified interval\n",
    "            stock = yf.Ticker(ticker)\n",
    "            \n",
    "            # Use yfinance's built-in period and interval parameters for efficiency\n",
    "            hist = stock.history(period=data_period, interval=interval)\n",
    "            \n",
    "            if len(hist) < 10:  # Need at least 10 data points\n",
    "                print(f\"Insufficient data for {ticker}, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Only resample if the interval doesn't match our target\n",
    "            if interval != resample_period and resample_period != 'no_resample':\n",
    "                hist_resampled = hist.resample(resample_period).agg({\n",
    "                    'Open': 'first',\n",
    "                    'High': 'max',\n",
    "                    'Low': 'min',\n",
    "                    'Close': 'last',\n",
    "                    'Volume': 'sum'\n",
    "                }).dropna()\n",
    "            else:\n",
    "                hist_resampled = hist.copy()\n",
    "            \n",
    "            if len(hist_resampled) < sequence_length + 1:\n",
    "                print(f\"Insufficient resampled data for {ticker} ({len(hist_resampled)} points), skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate future returns (next period)\n",
    "            hist_resampled['Future_Return'] = hist_resampled['Close'].shift(-1) / hist_resampled['Close'] - 1\n",
    "            \n",
    "            # Get financial data (cached to avoid repeated API calls)\n",
    "            try:\n",
    "                info = stock.info\n",
    "            except:\n",
    "                info = {}\n",
    "            \n",
    "            # Get company news (cached per ticker) - get 9 articles for 3 clusters of 3 each\n",
    "            company_name = company_names.get(ticker, ticker)\n",
    "            news_articles = news_collector.get_company_news(company_name, ticker, num_articles=9)\n",
    "            \n",
    "            # Create data points efficiently\n",
    "            sample_count = 0\n",
    "            available_samples = len(hist_resampled) - sequence_length - 1\n",
    "            \n",
    "            # Sample indices to get diverse data points across the time period\n",
    "            if available_samples > num_samples_per_ticker:\n",
    "                sample_indices = np.random.choice(available_samples, num_samples_per_ticker, replace=False)\n",
    "                sample_indices = sorted(sample_indices + sequence_length)  # Adjust for sequence length\n",
    "            else:\n",
    "                sample_indices = range(sequence_length, len(hist_resampled) - 1)\n",
    "            \n",
    "            for i in sample_indices:\n",
    "                if not np.isnan(hist_resampled['Future_Return'].iloc[i]) and sample_count < num_samples_per_ticker:\n",
    "                    # Price data (last sequence_length points)\n",
    "                    start_idx = max(0, i - sequence_length + 1)\n",
    "                    price_data = hist_resampled['Close'].iloc[start_idx:i+1].tolist()\n",
    "                    \n",
    "                    # Pad if necessary (shouldn't happen with our logic above, but safety check)\n",
    "                    if len(price_data) < sequence_length:\n",
    "                        price_data = [price_data[0]] * (sequence_length - len(price_data)) + price_data\n",
    "                    \n",
    "                    # Financial metrics (using cached info)\n",
    "                    financial_metrics = [\n",
    "                        np.random.normal(0, 0.1),  # Revenue growth (simulated)\n",
    "                        np.random.normal(0, 0.1),  # Net income growth (simulated)\n",
    "                        np.random.normal(0, 0.1),  # EPS growth (simulated)\n",
    "                        info.get('trailingPE', 15) / 100 if info.get('trailingPE') else 0.15,  # Normalized P/E\n",
    "                        np.random.normal(0, 0.1),  # Debt-to-equity (simulated)\n",
    "                        np.random.normal(0, 0.1)   # ROE (simulated)\n",
    "                    ]\n",
    "                    \n",
    "                    # Select a random news article for this sample\n",
    "                    import random\n",
    "                    selected_news = random.choice(news_articles) if news_articles else f\"{company_name} financial update.\"\n",
    "                    \n",
    "                    # Classify the return into 5 categories\n",
    "                    target_class = classify_return(hist_resampled['Future_Return'].iloc[i])\n",
    "                    \n",
    "                    data.append({\n",
    "                        'ticker': ticker,\n",
    "                        'company_name': company_name,\n",
    "                        'date': hist_resampled.index[i],\n",
    "                        'price_data': price_data,\n",
    "                        'financial_metrics': financial_metrics,\n",
    "                        'news_text': selected_news,\n",
    "                        'target': target_class,\n",
    "                        'return_pct': hist_resampled['Future_Return'].iloc[i]\n",
    "                    })\n",
    "                    sample_count += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {str(e)}\")\n",
    "            \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Configuration options for different training scenarios\n",
    "TRAINING_CONFIGS = {\n",
    "    # Option 1: 6 months, weekly data, resampled to 15-day (most efficient)\n",
    "    'efficient_6mo': {\n",
    "        'data_period': '6mo',\n",
    "        'interval': '1wk',  # Get weekly data from API (efficient)\n",
    "        'resample_period': '15D',  # Resample to 15-day intervals\n",
    "        'sequence_length': 12,  # Adjust sequence length to available data\n",
    "        'description': '6 months of weekly data, resampled to 15-day intervals'\n",
    "    },\n",
    "    \n",
    "    # Option 2: 1 year, weekly data, resampled to 15-day (balanced)\n",
    "    'balanced_1y': {\n",
    "        'data_period': '1y',\n",
    "        'interval': '1wk',\n",
    "        'resample_period': '15D',\n",
    "        'sequence_length': 24,  # Can use full sequence length\n",
    "        'description': '1 year of weekly data, resampled to 15-day intervals'\n",
    "    },\n",
    "    \n",
    "    # Option 3: 6 months, daily data, resampled to 15-day (more granular but less efficient)\n",
    "    'detailed_6mo': {\n",
    "        'data_period': '6mo',\n",
    "        'interval': '1d',\n",
    "        'resample_period': '15D',\n",
    "        'sequence_length': 12,\n",
    "        'description': '6 months of daily data, resampled to 15-day intervals'\n",
    "    },\n",
    "    \n",
    "    # Option 4: Original approach (2 years of data)\n",
    "    'original_2y': {\n",
    "        'data_period': '2y',\n",
    "        'interval': '1d',\n",
    "        'resample_period': '15D',\n",
    "        'sequence_length': 24,\n",
    "        'description': '2 years of daily data, resampled to 15-day intervals (original)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tech companies list (20 companies)\n",
    "tech_companies = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX', \n",
    "    'ADBE', 'CRM', 'ORCL', 'IBM', 'INTC', 'AMD', 'CSCO', 'PYPL', \n",
    "    'UBER', 'LYFT', 'SNAP', 'TWTR'\n",
    "]\n",
    "\n",
    "# Choose configuration - recommend 'balanced_1y' for best results\n",
    "SELECTED_CONFIG = 'balanced_1y'  # Change this to 'efficient_6mo' if you prefer 6 months\n",
    "config = TRAINING_CONFIGS[SELECTED_CONFIG]\n",
    "\n",
    "# Set your Google Custom Search API credentials here if you want to use real news data\n",
    "GOOGLE_API_KEY = None  # Replace with your actual API key: \"your_google_api_key_here\"\n",
    "SEARCH_ENGINE_ID = None  # Replace with your actual Search Engine ID: \"your_search_engine_id_here\"\n",
    "\n",
    "print(f\"Using configuration: {SELECTED_CONFIG}\")\n",
    "print(f\"Description: {config['description']}\")\n",
    "print(f\"Data period: {config['data_period']}\")\n",
    "print(f\"API interval: {config['interval']}\")\n",
    "print(f\"Resample period: {config['resample_period']}\")\n",
    "print(f\"Sequence length: {config['sequence_length']}\")\n",
    "print(f\"Using {'real' if GOOGLE_API_KEY and SEARCH_ENGINE_ID else 'synthetic'} news data\")\n",
    "print()\n",
    "\n",
    "# Collect training data with selected configuration\n",
    "training_data = collect_training_data_efficient(\n",
    "    ticker_list=tech_companies,\n",
    "    data_period=config['data_period'],\n",
    "    interval=config['interval'],\n",
    "    resample_period=config['resample_period'],\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    search_engine_id=SEARCH_ENGINE_ID,\n",
    "    num_samples_per_ticker=50,  # Increased from 30\n",
    "    sequence_length=config['sequence_length']\n",
    ")\n",
    "\n",
    "print(f\"Collected {len(training_data)} samples from {training_data['ticker'].nunique()} companies\")\n",
    "print(f\"Expected sequence length in price data: {config['sequence_length']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the diversity of our collected data\n",
    "print(\"Sample of collected training data:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show unique companies\n",
    "print(f\"Companies: {sorted(training_data['ticker'].unique())}\")\n",
    "print(f\"Total companies: {training_data['ticker'].nunique()}\")\n",
    "print(f\"Total samples: {len(training_data)}\")\n",
    "print(f\"Samples per company: {training_data['ticker'].value_counts().describe()}\")\n",
    "\n",
    "print(\"\\nSample news articles (first 5):\")\n",
    "print(\"-\" * 40)\n",
    "for i, (idx, row) in enumerate(training_data.head().iterrows()):\n",
    "    print(f\"{i+1}. {row['ticker']} - {row['company_name']}\")\n",
    "    print(f\"   News: {row['news_text']}\")\n",
    "    print(f\"   Target: {'INCREASE' if row['target'] == 1 else 'DECREASE'}\")\n",
    "    print()\n",
    "\n",
    "# Check news diversity\n",
    "unique_news = training_data['news_text'].nunique()\n",
    "total_samples = len(training_data)\n",
    "print(f\"News diversity: {unique_news} unique articles out of {total_samples} samples\")\n",
    "print(f\"Diversity ratio: {unique_news/total_samples:.2%}\")\n",
    "\n",
    "# Target distribution\n",
    "target_dist = training_data['target'].value_counts().sort_index()\n",
    "class_names = ['üìâ Strong Decrease', 'üîª Moderate Decrease', '‚û°Ô∏è Stable', 'üî∫ Moderate Increase', 'üìà Strong Increase']\n",
    "\n",
    "print(f\"\\nTarget distribution (5-class system):\")\n",
    "for i in range(5):\n",
    "    count = target_dist.get(i, 0)\n",
    "    percentage = count/len(training_data)*100 if len(training_data) > 0 else 0\n",
    "    print(f\"{class_names[i]} ({i}): {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Show return ranges for each class\n",
    "print(f\"\\nReturn ranges by class:\")\n",
    "for i in range(5):\n",
    "    class_data = training_data[training_data['target'] == i]\n",
    "    if len(class_data) > 0:\n",
    "        min_ret = class_data['return_pct'].min() * 100\n",
    "        max_ret = class_data['return_pct'].max() * 100\n",
    "        avg_ret = class_data['return_pct'].mean() * 100\n",
    "        print(f\"{class_names[i]}: {min_ret:.2f}% to {max_ret:.2f}% (avg: {avg_ret:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, sequence_length=24):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        \n",
    "        # Process price data - ensure it matches expected sequence length\n",
    "        price_data = torch.tensor(item['price_data'], dtype=torch.float32)\n",
    "        \n",
    "        # Pad or truncate to match expected sequence length\n",
    "        if len(price_data) < self.sequence_length:\n",
    "            # Pad with the first value\n",
    "            padding = torch.full((self.sequence_length - len(price_data),), price_data[0])\n",
    "            price_data = torch.cat([padding, price_data])\n",
    "        elif len(price_data) > self.sequence_length:\n",
    "            # Take the last sequence_length values\n",
    "            price_data = price_data[-self.sequence_length:]\n",
    "        \n",
    "        # Normalize price data\n",
    "        price_data = (price_data - price_data.mean()) / (price_data.std() + 1e-8)\n",
    "        \n",
    "        # Process news data\n",
    "        news_inputs = self.tokenizer(\n",
    "            item['news_text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Process financial data\n",
    "        financial_data = torch.tensor(item['financial_metrics'], dtype=torch.float32)\n",
    "        financial_data = (financial_data - financial_data.mean()) / (financial_data.std() + 1e-8)\n",
    "        \n",
    "        return {\n",
    "            'price_data': price_data,\n",
    "            'news_input_ids': news_inputs['input_ids'].squeeze(),\n",
    "            'news_attention_mask': news_inputs['attention_mask'].squeeze(),\n",
    "            'financial_data': financial_data,\n",
    "            'target': torch.tensor(item['target'], dtype=torch.long)  # Changed to long for CrossEntropyLoss\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets with sequence length from config\n",
    "train_dataset = StockDataset(train_data, tokenizer, sequence_length=config['sequence_length'])\n",
    "val_dataset = StockDataset(val_data, tokenizer, sequence_length=config['sequence_length'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Using sequence length: {config['sequence_length']}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_model(train_loader, val_loader, sequence_length=24, num_epochs=5, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on device: {device}\")\n",
    "    \n",
    "    # Initialize ensemble model for 5-class classification with configurable sequence length\n",
    "    model = EnsembleModel(num_classes=5).to(device)\n",
    "    # Update the price model's sequence length\n",
    "    model.price_model.sequence_length = sequence_length\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()  # Changed from BCELoss to CrossEntropyLoss\n",
    "    \n",
    "    # Class names for reporting\n",
    "    class_names = ['üìâ Strong Decrease', 'üîª Moderate Decrease', '‚û°Ô∏è Stable', 'üî∫ Moderate Increase', 'üìà Strong Increase']\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            price_data = batch['price_data'].to(device)\n",
    "            news_input_ids = batch['news_input_ids'].to(device)\n",
    "            news_attention_mask = batch['news_attention_mask'].to(device)\n",
    "            financial_data = batch['financial_data'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(news_input_ids, news_attention_mask, financial_data, price_data)\n",
    "            loss = criterion(outputs, targets)  # No need to squeeze for CrossEntropyLoss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with highest probability\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                price_data = batch['price_data'].to(device)\n",
    "                news_input_ids = batch['news_input_ids'].to(device)\n",
    "                news_attention_mask = batch['news_attention_mask'].to(device)\n",
    "                financial_data = batch['financial_data'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(news_input_ids, news_attention_mask, financial_data, price_data)\n",
    "                loss = criterion(outputs, targets)  # No need to squeeze for CrossEntropyLoss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)  # Get the class with highest probability\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Print epoch results\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        print(f'\\\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'epoch': epoch\n",
    "            }, 'best_ensemble_model.pth')\n",
    "            print(f'New best model saved with validation loss: {val_loss:.4f}')\n",
    "        \n",
    "        print('-' * 50)\n",
    "            \n",
    "    return model\n",
    "\n",
    "# Train the ensemble model\n",
    "print(\"Starting training...\")\n",
    "trained_model = train_ensemble_model(train_loader, val_loader, sequence_length=config['sequence_length'], num_epochs=5)\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Save Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'model_config': {\n",
    "        'news_model': 'distilbert-base-uncased',\n",
    "        'financial_input_size': 6,\n",
    "        'price_sequence_length': config['sequence_length'],\n",
    "        'num_classes': 5,\n",
    "        'training_config': SELECTED_CONFIG\n",
    "    }\n",
    "}, '../models/ensemble_model.pth')\n",
    "\n",
    "# Also save individual model components\n",
    "torch.save(trained_model.news_model.state_dict(), '../models/news_model.pth')\n",
    "torch.save(trained_model.financial_model.state_dict(), '../models/financial_model.pth')\n",
    "torch.save(trained_model.price_model.state_dict(), '../models/price_model.pth')\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- ../models/ensemble_model.pth\")\n",
    "print(\"- ../models/news_model.pth\")\n",
    "print(\"- ../models/financial_model.pth\")\n",
    "print(\"- ../models/price_model.pth\")\n",
    "print(\"- best_ensemble_model.pth (best checkpoint)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
